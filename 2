package com.mapreduce.classes;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;


public class Onida_total {
    public static IntWritable one = new IntWritable(1);
    public static class groupingMapper extends Mapper<LongWritable, Text, Text,IntWritable>{

        //private static final Logger _logger = Logger.getLogger(FilteringMapper.class);

        public void map(LongWritable key, Text rec, Context con) {
            
            String record = rec.toString();
            String[] s = record.split("\\|");
            String companyname = s[0];
            String statename = s[3];
            if((companyname.equalsIgnoreCase("Onida")))
            {
                try {
                    con.write(new Text(statename),one);
                } catch (IOException e) {
                    // TODO Auto-generated catch block
                    e.printStackTrace();
                } catch (InterruptedException e) {
                    // TODO Auto-generated catch block
                    e.printStackTrace();
                }
            }
        }
    }
    public static class groupingReducer extends Reducer<Text, IntWritable, Text,IntWritable>{

        //private static final Logger _logger = Logger.getLogger(FilteringMapper.class);

        public void reduce(Text key, Iterable<IntWritable> values, Context context) {
            
            int count = 0;
            IntWritable result = new IntWritable();
            {
                try {
                    for (IntWritable val : values) {
                    
                        count+= val.get();
                    }
                    result.set(count);
                    context.write(key,result);
                } catch (IOException e) {
                    // TODO Auto-generated catch block
                    e.printStackTrace();
                } catch (InterruptedException e) {
                    // TODO Auto-generated catch block
                    e.printStackTrace();
                }
            }
        }
    }
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
         
            Configuration conf = new Configuration();
                   
             @SuppressWarnings("deprecation")
             
               Job job = new Job(conf, "filterdata");
               
               job.setJarByClass(groupingMapper.class);
               
               job.setOutputKeyClass(Text.class);
               
               job.setOutputValueClass(IntWritable.class);
                   
               job.setMapperClass(groupingMapper.class);
               
               job.setReducerClass(groupingReducer.class);
                                      
               job.setInputFormatClass(TextInputFormat.class);
               
               job.setOutputFormatClass(TextOutputFormat.class);
                   
               FileInputFormat.addInputPath(job, new Path(args[0]));
               
               FileOutputFormat.setOutputPath(job, new Path(args[1]));
                   
               job.waitForCompletion(true);

}
}
